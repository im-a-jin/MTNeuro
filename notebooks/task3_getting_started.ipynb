{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started: Multi-Task Decoding (Task 3)\n",
    "\n",
    "\n",
    "<br>\n",
    "<a href=\"https://colab.research.google.com/github/MTNeuro/MTNeuro/blob/main/notebooks/task3_getting_started.ipynb\">\n",
    "    <img align=\"left\" alt=\"Open in Colab button\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"150\" height=\"60\">\n",
    "</a>\n",
    "<br>    \n",
    "\n",
    "This **MTNeuro** jupyter notebook takes you through how you can execute `task 3`. It takes in an encoder and computes R2 scores between embeddings and different Semantic features.\n",
    "\n",
    "For more details on the tasks and dataset, please refer to our paper:\n",
    "\n",
    "    \"Quesada, J., Sathidevi, L., Liu, R., Ahad, N., Jackson, J.M., Azabou, M., ... & Dyer, E. L. (2022). MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction. Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone MTNeuro repo and Install the MTNeuro package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/MTNeuro/MTNeuro && cd MTNeuro && pip install -e .\n",
    "!pip install pandas\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import umap\n",
    "\n",
    "#PyTorch imports\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Sci-kit learn imports\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#MTNeuro modules\n",
    "#sys.path.append('./MTNeuro')                 #setting the location to look for the required packages\n",
    "from MTNeuro.annots.features import extract_cell_stats,extract_axon_stats,extract_blood_stats\n",
    "from MTNeuro.annots.get_cutouts import get_cutout_data\n",
    "from MTNeuro.annots.latents import get_latents, get_unsup_latents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Encoder\n",
    "\n",
    "The encoder file path **and** the encoder type is required here. \n",
    "\n",
    "Any of the models used in Task 1 can be used as an encoder. The model weight can be found here: [[Dropbox](https://www.dropbox.com/sh/jkk1i9wopqqrgne/AABHDICD0Cfwl_wm5q2ueIS8a)]\n",
    "\n",
    "The encoder types are `ssl`, `supervised`, `PCA`, and `NMF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this demo, BYOL is used. \n",
    "#TODO: Download the model weights for BYOL from the Dropbox and place the file in the same directory as this notebook.\n",
    "!wget https://www.dropbox.com/s/htjw410bk0grhj4/ckpt-800.pt\n",
    "\n",
    "encoder_file_path = \"ckpt-800.pt\"\n",
    "encoder_type = \"ssl\"\n",
    "\n",
    "#--Do not motify below--\n",
    "if encoder_type == 'ssl':\n",
    "    ssl_encoder = 1\n",
    "    unsupervised = 0\n",
    "elif encoder_type == 'supervised':\n",
    "    ssl_encoder =  0\n",
    "    unsupervised = 0\n",
    "elif encoder_type == 'PCA': \n",
    "    unsupervised = 1\n",
    "    set_pca = 1\n",
    "elif encoder_type == 'NMF':\n",
    "    unsupervised = 1\n",
    "    set_pca = 0\n",
    "else:\n",
    "    print(\"Incorrectly specified encoder type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Cutout Coordinates\n",
    "\n",
    "Cutout coordinates are specified in the task 3 JSON file found at MTNeuro/taskconfig/task3.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = \"../MTNeuro/taskconfig/task3.json\"\n",
    "\n",
    "try:\n",
    "    jsonFile = open(config_file_path, 'r')\n",
    "    slices = json.load(jsonFile)\n",
    "except IOError:\n",
    "    print(\"JSON file not found.\")\n",
    "jsonFile.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Annotations Specified in JSON File\n",
    "\n",
    "Each cutout from each region of the brain has shape (Z,Y,X) = (360,256,256). \n",
    "\n",
    "All 4 cutouts are concatenated together along the z-axis to form `data_array_raw` and `data_array_anno`, which both have shape (1440,256,256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving Data from JSON\n",
    "xrange_list = [slices['xrange_cor'],slices['xrange_stri'],slices['xrange_vp'],slices['xrange_zi']]\n",
    "yrange_list = [slices['yrange_cor'],slices['yrange_stri'],slices['yrange_vp'],slices['yrange_zi']]\n",
    "class_list = [\"Cortex\",\"Striatum\",\"VP\",\"ZI\"]\n",
    "zrange = slices['zrange']\n",
    "\n",
    "\n",
    "boss_dict = {}\n",
    "boss_dict['image_chan']=slices['image_chan']\n",
    "boss_dict['annotation_chan'] = slices['annotation_chan']\n",
    "\n",
    "data_array_raw = []\n",
    "data_array_anno = []\n",
    "label_array  = []\n",
    "up_sample = 4 \n",
    "\n",
    "#Pulling Data from BossDB\n",
    "for i in range(0,len(xrange_list)):\n",
    "    cutout_data_raw,cutout_data_anno = get_cutout_data(xrange_list[i],yrange_list[i],zrange,name=class_list[i])\n",
    "    \n",
    "    data_raw = cutout_data_raw[:,:,:]\n",
    "    data_anno = cutout_data_anno[:,:,:]\n",
    "    data_array_raw = np.concatenate((data_array_raw,data_raw),axis =0 ) if len(data_array_raw) else data_raw \n",
    "    \n",
    "    data_array_anno = np.concatenate((data_array_anno,data_anno),axis =0 ) if len(data_array_anno) else data_anno\n",
    "    \n",
    "    labels = i*np.ones(up_sample*len(data_raw)).reshape(-1,)\n",
    "    label_array  = np.concatenate((label_array ,labels),axis =0) if len(labels) else labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features and Calculating Linear Readout Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Extracting cell stats...')\n",
    "stats_cell= extract_cell_stats(np.copy(data_array_anno))\n",
    "\n",
    "print('Extracting axon stats...')\n",
    "stats_axon = extract_axon_stats(np.copy(data_array_anno))\n",
    "\n",
    "print('Extracting blood stats...')\n",
    "stats_blood = extract_blood_stats(np.copy(data_array_anno))\n",
    "\n",
    "'get results for different encoders'\n",
    "if encoder_type == 'ssl' or encoder_type == 'supervised':\n",
    "    embeddings = get_latents(data_array_raw,encoder_file_path,ssl_encoder)\n",
    "elif encoder_type == 'PCA' or encoder_type == 'NMF':\n",
    "    embeddings = get_unsup_latents(data_array_raw,set_pca)\n",
    "\n",
    "'Get linear readout scores'\n",
    "X = embeddings\n",
    "\n",
    "y = stats_blood[:,1]\n",
    "reg = LinearRegression().fit(X,y)\n",
    "blood_vsl_score = reg.score(embeddings,stats_blood[:,1])\n",
    "print(\"Blood Vessel Score : {}\".format(blood_vsl_score ))\n",
    "\n",
    "y = stats_cell[:,1]\n",
    "reg = LinearRegression().fit(X,y)\n",
    "numb_cell = reg.score(embeddings,stats_cell[:,1])\n",
    "print(\"Cell Count Score:{}\".format(numb_cell))\n",
    "\n",
    "y = stats_cell[:,2]\n",
    "reg = LinearRegression().fit(X,y)\n",
    "avg_dist_nn_cell = reg.score(embeddings,stats_cell[:,2])\n",
    "print(\"Avg Cell Distance Score :{}\".format(avg_dist_nn_cell ))\n",
    "\n",
    "y = stats_cell[:,4]\n",
    "reg = LinearRegression().fit(X,y)\n",
    "cell_size = reg.score(embeddings,stats_cell[:,4])\n",
    "print(\"Cell Size Score:{}\".format(cell_size))\n",
    "\n",
    "y = stats_axon[:,1]\n",
    "reg = LinearRegression().fit(X,y)\n",
    "axon_rslt = reg.score(embeddings,stats_axon[:,1])\n",
    "print(\"Axon Score: {}\".format(axon_rslt ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f4108ecf01062aca25e5dc86ed0d2b2584059290a9a8508cbbea0875ac6d25a"
  },
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
