{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee34b92",
   "metadata": {},
   "source": [
    "# Getting Started: Pixel-Level Segmentation (Task 2)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MTNeuro/MTNeuro/blob/notebook_cleanup/notebooks/task2_getting_started.ipynb)\n",
    "\n",
    "This notebook walks through the process of loading in the volume cutouts from BossDB (with visualization), training UNet, and evaluating the model with performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161da36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install Pytorch for CUDA 11.3\n",
    "# !pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2775161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install intern[cloudvolume] scikit-learn timm pretrainedmodels efficientnet_pytorch segmentation-models-pytorch\n",
    "!git clone https://github.com/MTNeuro/MTNeuro && pip install .\n",
    "%cd MTNeuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1958fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import json as json\n",
    "from tqdm.notebook import tqdm\n",
    "import segmentation_models_pytorch as smp      \n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# BossDB library and MTNeuro dataset loader\n",
    "from intern import array\n",
    "from MTNeuro.bossdbdataset import BossDBDataset               \n",
    "from MTNeuro.trainer import Trainer                    \n",
    "from MTNeuro.models.unet import UNet                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be89db",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "Task 2 (Microstructure Segmentation) is trained with 4 three-dimensional 256 × 256 × 360 cutouts from the somatosensory cortex (CTX), striatum (STR), ventral posterior region of thalamus (VP), and the zona incerta (ZI) from a high-resolution (1.17μm isotropic) 3D microscopy database. These volumetric cutouts contain voxel-level microstructural labels, identifying each voxel as either part of an axon, cell, blood vessel, or background. [[1]](#1)\n",
    "\n",
    "<img src=\"https://mtneuro.github.io/images/dataset.png\" alt=\"Dataset\" style=\"width: 70%;\">\n",
    "<img src=\"https://mtneuro.github.io/images/dataset2.png\" alt=\"Microstructures\" style=\"width: 70%;\">\n",
    "\n",
    "The microstructural annotations are labeled as:\n",
    "- 0: Background\n",
    "- 1: Blood Vessels\n",
    "- 2: Cells\n",
    "- 3: Axons\n",
    "\n",
    "The dataset and all corresponding labels are stored publicly in [BossDB](https://bossdb.org/project/prasad2020) and accessed on-demand with [Intern](https://pypi.org/project/intern/), a Python API library.\n",
    "\n",
    "## Task Description\n",
    "\n",
    "Task 2 evaluates the ability of baseline models to predict pixel-specific microstructure from the 4 brain regions specified above. In a 3-class setting, the cutouts are segmented into blood vessels, cells, and background + axons. In a 4-class setting, the cutouts are segmented to blood vessels, cells, axons, and background. Since the background and axons of the zona incerta (ZI) are virtually indistinguishable to even trained anatomists, we group axons and background in one class for the 3-class setting, and the 4-class setting is not trained on the zona incerta (ZI). Task 2 can be trained on 2 dimensional cutout images or on 3 dimensional cutout volumes. [[2]](#2)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MTNeuro/MTNeuro/notebook_cleanup/notebooks/images/task2.png\" alt=\"Task 2\" style=\"width: 50%;\">\n",
    "\n",
    "#### Train/Test Split\n",
    "\n",
    "We use the same train and test split as in ROI-C1 in Task 1: 300 images for train and 50 images for test, with a gap of 10 slices between datasets on the main subvolumes that are densely annotated at the pixel level. [[2]](#2)\n",
    "\n",
    "|        | Train | Buffer  | Test    |\n",
    "|:------:|:-----:|:-------:|:-------:|\n",
    "| Slices | 0-299 | 300-309 | 310-359 |\n",
    "\n",
    "All slices denoted as \"train\" are concatenated to form the train dataset, and all slices denoted as \"test\" are concatenated to form the test dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"1\">[1]</a> Prasad, J. A., Balwani, A. H., Johnson, E. C., Miano, J. D., Sampathkumar, V., De Andrade, V., ... & Dyer, E. L. (2020). A three-dimensional thalamocortical dataset for characterizing brain heterogeneity. Scientific Data, 7(1), 1-7.\n",
    "\n",
    "<a id=\"2\">[2]</a> Quesada, J., Sathidevi, L., Liu, R., Ahad, N., Jackson, J.M., Azabou, M., ... & Dyer, E. L. (2022). MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction. Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\n",
    "\n",
    "## Specify the Dimensions to Train and the Number of Classes Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = 2      # 2 or 3\n",
    "class_number = 4    # 3 or 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff18e7f",
   "metadata": {},
   "source": [
    "## Downloading the Volume Cutouts with BossDB\n",
    "\n",
    "An optional transformation of the data cutouts can be specified here. If no transformations are specified, the default toTensor() transformation is necessary since the PyTorch DataLoader requires a tensor as input.\n",
    "\n",
    "The script for the BossDBDataset can be found [here](https://github.com/MTNeuro/MTNeuro/blob/main/MTNeuro/bossdbdataset.py).\n",
    "\n",
    "A BossDBDataset object is initialized with the task_config dictionary, a boss_config dictionary (here passed in as `None`), the mode of data (`\"train\"`, `\"test\"`, `\"val\"`), and image and mask transform. A BossDBDataset object can be passed into a PyTorch DataLoader to properly split and load in the data into batches for training/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./MTNeuro/\"\n",
    "\n",
    "with open(os.path.join(root, f\"taskconfig/task2_{dimensions}D_{class_number}class.json\")) as file:\n",
    "    task_config = json.load(file)\n",
    "\n",
    "# Specify the transformations here:\n",
    "# transform = ...\n",
    "\n",
    "# If no transformations are used:\n",
    "transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "train_data =  BossDBDataset(task_config, None, 'train', image_transform = transform, mask_transform = transform)\n",
    "val_data = BossDBDataset(task_config, None, 'val', image_transform = transform, mask_transform = transform)\n",
    "test_data =  BossDBDataset(task_config, None, \"test\", image_transform = transform, mask_transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af11ca",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "#### Visualizing the First Slice from the First Batch and Its Groundtruth Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2643799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the input and the groundtruth annotations of the first batch\n",
    "input_data, annotations = train_data[0]\n",
    "\n",
    "#Data Characteristics\n",
    "print(f'Shape of input: {input_data.shape}')\n",
    "print(f'Shape of groundtruth annotations: {annotations.shape}')\n",
    "\n",
    "legend_elements = [Patch(facecolor='grey', label='Background'),\n",
    "                   Patch(facecolor='yellow', label='Cell'),\n",
    "                   Patch(facecolor='red', label='Blood Vessel')] if class_number == 3 else [Patch(facecolor='grey', label='Background'),\n",
    "                   Patch(facecolor='yellow', label='Cell'),\n",
    "                   Patch(facecolor='red', label='Blood Vessel'),\n",
    "                   Patch(facecolor='cyan', label='Axon')]\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
    "ax1.imshow(input_data[0], cmap='gray')\n",
    "ax1.title.set_text(\"The First Slice\")\n",
    "ax1.axis('off')\n",
    "\n",
    "cmap = ListedColormap(['grey', 'red', 'yellow', 'cyan'])\n",
    "ax2.imshow(annotations.squeeze(), vmin=0, vmax=3, cmap = cmap)\n",
    "ax2.title.set_text(\"It's Groundtruth Annotation\")\n",
    "ax2.axis('off')\n",
    "ax2.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa15074",
   "metadata": {},
   "source": [
    "#### A Collage of Random Slices from the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bfdf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data and select a random subset of 100 slices\n",
    "if dimensions==3:\n",
    "    reshaped_data = []\n",
    "    for data in train_data:\n",
    "        reshaped_data = reshaped_data + list(data[0][0])\n",
    "if dimensions==2:\n",
    "    reshaped_data = [data[0][0] for data in train_data]\n",
    "    \n",
    "random_samples_from_data = random.sample(reshaped_data,100)\n",
    "    \n",
    "grid_img = make_grid(torch.stack(random_samples_from_data).unsqueeze(1), nrow=10)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(grid_img[0], cmap='gray')\n",
    "plt.title(\"A Collage of 100 Random Slices\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62905c",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "This notebook uses UNet as an example baseline model on which to execute Task 2. UNet, a modified fully convolutional neural network that is optimized for biomedical image segmentation, features a U-shaped symmetric contracting and expanding path which can quickly segment all pixels of an input image. [[3]](#3)\n",
    "\n",
    "See a tutorial and walkthrough of UNet [here](https://github.com/johschmidt42/PyTorch-2D-3D-UNet-Tutorial).\n",
    "See the paper that details UNet's architecture and usage [here](https://arxiv.org/pdf/1505.04597.pdf). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MTNeuro/MTNeuro/notebook_cleanup/notebooks/images/unet.PNG\" alt=\"UNet\" style=\"width: 80%;\">\n",
    "\n",
    "<a id=\"3\" href=\"https://arxiv.org/pdf/1505.04597.pdf\" target=\"_blank\">[3]</a> Ronneberger, O., Fischer, P., & Brox T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. Medical Image Computing and Computer Assisted Intervention Society.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Load the appropriate network configurations JSON files\n",
    "\n",
    "with open(os.path.join(root, f\"networkconfig/UNet_{dimensions}D_{class_number}class.json\")) as file:\n",
    "    network_config = json.load(file)\n",
    "\n",
    "# Load the model with the configuration specified through the network configurations file.\n",
    "print('Loading UNet Model')\n",
    "model = UNet(in_channels = network_config['in_channels'],\n",
    "             out_channels = network_config['classes'],\n",
    "             n_blocks = network_config['n_blocks'],\n",
    "             start_filters = network_config['start_filters'],\n",
    "             activation = network_config['activation'],\n",
    "             normalization = network_config['normalization'],\n",
    "             conv_mode = network_config['conv_mode'],\n",
    "             dim = network_config['dim']\n",
    "            ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a28882",
   "metadata": {},
   "source": [
    "#### Setting the Loss Function and the Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Utilizes the network config file)\n",
    "if network_config[\"optimizer\"] == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=network_config[\"learning_rate\"])\n",
    "if network_config[\"optimizer\"] == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=network_config[\"learning_rate\"], betas=(network_config[\"beta1\"],network_config[\"beta2\"]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1251bc04",
   "metadata": {},
   "source": [
    "#### Configuring the DataLoader\n",
    "\n",
    "Documentation on the PyTorch DataLoader can be found [here](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f74339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=network_config['batch_size'],\n",
    "                              shuffle=False)\n",
    "validation_dataloader = DataLoader(dataset=val_data,\n",
    "                                   batch_size=network_config['batch_size'],\n",
    "                                   shuffle=False)\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=network_config['batch_size'],\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6de59",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "The Trainer object assists with training a specified model with data that is feeded in through a DataLoader. Some of its initialization arguments are specified from this notebook, and some are specified through the network configuration JSON file. \n",
    "\n",
    "Trainer.run_trainer() initiates the model training process.\n",
    "\n",
    "The script for Trainer can be found [here](https://github.com/MTNeuro/MTNeuro/blob/main/MTNeuro/trainer.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d279a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_DataLoader=train_dataloader,\n",
    "                  validation_DataLoader=validation_dataloader,\n",
    "                  epochs=network_config[\"epochs\"],\n",
    "                  notebook=True)\n",
    "\n",
    "training_losses, validation_losses, lr_rates = trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1939e",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47383372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img, model, device):\n",
    "    \"\"\"Segments the pixels of a slice using a trained model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = img.to(device)                     #send input to device\n",
    "    with torch.inference_mode():\n",
    "        out = model(x)                     #model forward pass\n",
    "    out_argmax = torch.argmax(out, dim=1)  #perform softmax on outputs\n",
    "    return out_argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112b9aa",
   "metadata": {},
   "source": [
    "#### Model Prediction\n",
    "\n",
    "The trained model is tested with the test_dataloader that was initialized when the test dataset was downloaded from BossDB. We keep track of the true positive, false positive, false negative, and true negative scores in tensors for use later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for storing the True Postive, False Positive, False Negative and True Negative scores respectively\n",
    "tp_tot = torch.empty(0,network_config['classes'])\n",
    "fp_tot = torch.empty(0,network_config['classes'])\n",
    "fn_tot = torch.empty(0,network_config['classes'])\n",
    "tn_tot = torch.empty(0,network_config['classes'])\n",
    "\n",
    "# Predict on test data\n",
    "for x, y in test_dataloader:\n",
    "    target = y.to(device) \n",
    "    # Segment the input image with the model\n",
    "    output = predict(x, model, device)\n",
    "    # Using segmentation_models_pytorch to calculate the statistics\n",
    "    tp, fp, fn, tn = smp.metrics.get_stats(output, target, mode='multiclass', num_classes = network_config['classes'])\n",
    "    tp_tot = torch.vstack((tp_tot,tp))\n",
    "    fp_tot = torch.vstack((fp_tot,fp))\n",
    "    fn_tot = torch.vstack((fn_tot,fn))\n",
    "    tn_tot = torch.vstack((tn_tot,tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e56b4",
   "metadata": {},
   "source": [
    "#### Visualizing Model Predictions\n",
    "\n",
    "One slice, its groundtruth annotation, and the model's predicted annotations are visualized below. Visualizations use the same legend and cmap as the visualizations in above cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be18f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 15))\n",
    "ax1.imshow(x.squeeze()[0], cmap='gray')\n",
    "ax1.title.set_text(\"A Slice\")\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(y.squeeze()[0], vmin=0, vmax=3, cmap = cmap)  \n",
    "ax2.title.set_text(\"It's Groundtruth Annotation\")\n",
    "ax2.axis('off')\n",
    "ax2.legend(handles=legend_elements) \n",
    "\n",
    "ax3.imshow(output.squeeze()[0], vmin=0, vmax=3, cmap = cmap) \n",
    "ax3.title.set_text(\"Model Prediction\")\n",
    "ax3.axis('off')\n",
    "ax3.legend(handles=legend_elements) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d60481",
   "metadata": {},
   "source": [
    "#### Calculating Performance Metrics\n",
    "\n",
    "Metric Calculations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Accuracy: }&\\frac{tp + tn}{fp + tn + fn + tp}\\\\\\\\\n",
    "\\text{Balanced Accuracy: }&\\frac{\\text{Specificity} + \\text{Sensitivity}}{2}\\\\\\\\\n",
    "\\text{F1-Score: }&\\frac{2\\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\\\\\\\\\n",
    "\\text{IoU: }&\\frac{tp}{fp + fn + tp}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where tp = true positive, tn = true negative, fp = false positive, and fn = false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "acc = (tp_tot.mean(dim=0)+tn_tot.mean(dim=0))/(fp_tot.mean(dim=0)+tn_tot.mean(dim=0)+fn_tot.mean(dim=0)+tp_tot.mean(dim=0)) \n",
    "print('New Accuracy per Class:', np.array(acc.cpu()))\n",
    "\n",
    "# Balanced Accuracy\n",
    "spec =  (tn_tot[:,1:].mean())/(fp_tot[:,1:].mean()+tn_tot[:,1:].mean())\n",
    "sens =  (tp_tot[:,1:].mean())/(fn_tot[:,1:].mean()+tp_tot[:,1:].mean())\n",
    "balacc = (spec + sens)/2\n",
    "print(f'New Balanced Accuracy (No Background): {balacc}')\n",
    "\n",
    "# F1-score\n",
    "prec = tp_tot.mean(dim=0)/(fp_tot.mean(dim=0)+tp_tot.mean(dim=0))\n",
    "reca = tp_tot.mean(dim=0)/(fn_tot.mean(dim=0)+tp_tot.mean(dim=0))\n",
    "f1 = (2*reca*prec)/(reca+prec)\n",
    "print(f'New F1-Score: {np.array(f1.cpu())}\\nAvg. F1-Score: {f1.mean()}')\n",
    "\n",
    "# IoU\n",
    "iou = (tp_tot.mean(0))/(fp_tot.mean(0)+fn_tot.mean(0)+tp_tot.mean(0))\n",
    "print(f'New IoU: {np.array(iou.cpu())}\\nAvg. IoU-Score: {iou.mean()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f4108ecf01062aca25e5dc86ed0d2b2584059290a9a8508cbbea0875ac6d25a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
